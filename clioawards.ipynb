{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvYa1J75mDFDaRyHIFELow",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhuvanJoshi007/BhuvanJoshi007/blob/main/clioawards.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6p5twGKwQRs0"
      },
      "outputs": [],
      "source": [
        "# website to crawl: https://clios.com/awards/winners?page=23&year=2023&trophy=all, there are 24 pages.\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "url = \"https://clios.com/awards/winners?year=2023&trophy=all\"\n",
        "response = requests.get(url)\n",
        "\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Scrape elements with class \"container-cell-header row unwrap winners-info\"\n",
        "container_cells_trophy = soup.find_all('div', class_='container-cell-header row unwrap winners-info')\n",
        "# Scrape trophy information\n",
        "trophy=[]\n",
        "for container in container_cells_trophy:\n",
        "    trophy_divs = container.find_all('div', class_=lambda x: x and 'winners-trophy award' in x)\n",
        "    for trophy_div in trophy_divs:\n",
        "        trophy.append((trophy_div['class'][1]).split('-',1)[1])\n",
        "trophy_rank = pd.Series(trophy, name='Trophy_rank')\n",
        "\n",
        "# Scrape trophy information\n",
        "container_cells = soup.find_all('div', class_='container-cell row unwrap flat top-cell')\n",
        "locs = []\n",
        "yr = []\n",
        "for cell in container_cells:\n",
        "    location_tags = cell.find_all('p', class_='location')\n",
        "    year_tags = cell.find_all('p', class_='year')\n",
        "    for x in location_tags:\n",
        "        locs.append(x.text)\n",
        "    for y in year_tags:\n",
        "        yr.append(y.text)\n",
        "location= pd.Series(locs, name='Location')\n",
        "year= pd.Series(yr, name='Year')\n",
        "\n",
        "# Scrape winner information\n",
        "winners_divs = soup.find_all(\"div\", class_=\"container-cell-header row unwrap winners-info\")\n",
        "cmp = []\n",
        "ad=[]\n",
        "for div in winners_divs:\n",
        "    h6 = div.find(\"h6\", class_=\"winners-tag\").text.strip()\n",
        "    h5 = div.find(\"h5\", class_=\"winners-tag\").text.strip()\n",
        "    cmp.append(h6)\n",
        "    ad.append(h5)\n",
        "company = pd.Series(cmp, name='Company')\n",
        "Advertisement_title = pd.Series(ad, name='Advertisement_title')\n",
        "\n",
        "# Scrape elements with class \"container-cell-header row unwrap winners-info\"\n",
        "all_tr = soup.find_all('tr')\n",
        "\n",
        "th_l=[]\n",
        "td_l=[]\n",
        "for x in all_tr:\n",
        "    all_th = x.find_all('th')\n",
        "    all_td = x.find_all('td')\n",
        "    for th in all_th:\n",
        "      th_l.append(th.text)\n",
        "    for td in all_td:\n",
        "      td_l.append(td.text)\n",
        "Entrant_Company = pd.Series(td_l[0::4], name='Entrant_Company')\n",
        "Medium = pd.Series(td_l[1::4], name='Medium')\n",
        "Category = pd.Series(td_l[2::4],name= 'Category')\n",
        "Entry_Type = pd.Series(td_l[3::4], name='Entry_Type')\n",
        "\n",
        "df"
      ]
    }
  ]
}